{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQczX_il85nQ"
      },
      "source": [
        "**CS420 ASSIGNMENT 1 SOLUTIONS (CODE)**\n",
        "\n",
        "`NAME: KANG CHIN SHEN`\n",
        "\n",
        "`MATRIC ID: 01412921`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA-cU801C3cL"
      },
      "source": [
        "**Question 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIHaKUmHESdf"
      },
      "outputs": [],
      "source": [
        "# Install pgmpy on Google Collab\n",
        "!pip install pgmpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIQeyT5ODeRf"
      },
      "outputs": [],
      "source": [
        "# Import relevant packages\n",
        "from pgmpy.models import BayesianModel\n",
        "from pgmpy.factors.discrete import TabularCPD\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgpO5ZtjElwm"
      },
      "source": [
        "**Create the Bayesian network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La1wW5iEEoqe"
      },
      "outputs": [],
      "source": [
        "# Create a model which containts edges zof the graph\n",
        "model = BayesianModel([('Smoking', 'Yellow Fingers'),\n",
        "                       ('Smoking', 'Cancer'),\n",
        "                       ('Solar Flare', 'Radiation'), \n",
        "                       ('Microwave', 'Radiation'),\n",
        "                       ('Radiation', 'Cancer'),\n",
        "                       ('Radiation', 'Skin Burn'),\n",
        "                      ])\n",
        "\n",
        "# Enter conditional probability distribution for each variable:\n",
        "\n",
        "# Prior probability for Smoking\n",
        "cpd_smoking = TabularCPD(variable='Smoking', variable_card=2, values=[[0.9], [0.1]])\n",
        "\n",
        "# Prior probability for Solar Flare\n",
        "cpd_solar_flare = TabularCPD(variable='Solar Flare', variable_card=2, values=[[0.999], [0.001]])\n",
        "\n",
        "# Prior probability for Microwave\n",
        "cpd_microwave = TabularCPD(variable='Microwave', variable_card=2, values=[[0.001], [0.999]])\n",
        "\n",
        "# Conditional probability for P(Yellow Fingers | Smoking)\n",
        "cpd_yellow_fingers = TabularCPD(variable='Yellow Fingers', \n",
        "                               variable_card=2, \n",
        "                               values = [[0.9, 0.1],\n",
        "                                         [0.1, 0.9]],\n",
        "                               evidence = ['Smoking'],\n",
        "                               evidence_card=[2])\n",
        "\n",
        "# Conditional probability for P(Radiation | Solar Flare, Microwave)\n",
        "cpd_radiation = TabularCPD(variable='Radiation',\n",
        "                           variable_card=2, \n",
        "                           values = [[0.99, 0.8, 0.7, 0.5],\n",
        "                                     [0.01, 0.2, 0.3, 0.5]],\n",
        "                           evidence = ['Solar Flare', 'Microwave'],\n",
        "                           evidence_card=[2, 2])\n",
        "\n",
        "# Conditional probability for P(Cancer | Smoking, Radiation)\n",
        "cpd_cancer = TabularCPD(variable='Cancer',\n",
        "                           variable_card=2, \n",
        "                           values = [[0.9, 0.4, 0.8, 0.1],\n",
        "                                     [0.1, 0.6, 0.2, 0.9]],\n",
        "                           evidence = ['Smoking', 'Radiation'],\n",
        "                           evidence_card=[2, 2])\n",
        "\n",
        "# Conditional probability for P(Skin Burn | Radiation) \n",
        "cpd_skin_burn = TabularCPD(variable='Skin Burn', \n",
        "                        variable_card=2, \n",
        "                        values = [[0.99, 0.9],\n",
        "                                  [0.01, 0.1]],\n",
        "                        evidence = ['Radiation'],\n",
        "                        evidence_card=[2])\n",
        "\n",
        "# Insert the nodes and conditional probability into the defined model\n",
        "model.add_cpds(cpd_smoking,\n",
        "               cpd_solar_flare, \n",
        "               cpd_microwave, \n",
        "               cpd_yellow_fingers,\n",
        "               cpd_radiation,\n",
        "               cpd_cancer,\n",
        "               cpd_skin_burn)\n",
        "               "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6DmWVuej8VK"
      },
      "source": [
        "**Validate Network Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AH8I3YNffo6M"
      },
      "outputs": [],
      "source": [
        "# Validate the Bayesian Model\n",
        "print('Result of validation of constructed Bayesian Network: ' + str(model.check_model()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odaapYTEkrXT"
      },
      "outputs": [],
      "source": [
        "# Check the conditional probability associated with each node to ensure correctness\n",
        "\n",
        "print('Prior probability for Smoking')\n",
        "print(model.get_cpds('Smoking'))\n",
        "\n",
        "print('\\nPrior probability for Solar Flare')\n",
        "print(model.get_cpds('Solar Flare'))\n",
        "\n",
        "print('\\nPrior probability for Microwave')\n",
        "print(model.get_cpds('Microwave'))\n",
        "\n",
        "print('\\nConditional probability for P(Yellow Fingers | Smoking)')\n",
        "print(model.get_cpds('Yellow Fingers'))\n",
        "\n",
        "print('\\nConditional probability for P(Radiation | Solar Flare, Microwave)')\n",
        "print(model.get_cpds('Radiation'))\n",
        "\n",
        "print('\\nConditional probability for P(Cancer | Smoking, Radiation)')\n",
        "print(model.get_cpds('Cancer'))\n",
        "\n",
        "print('\\nConditional probability for P(Skin Burn | Radiation)')\n",
        "print(model.get_cpds('Skin Burn'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEyFtT9roC1C"
      },
      "outputs": [],
      "source": [
        "############### Inference ##################\n",
        "\n",
        "# Import VariableElimination library\n",
        "from pgmpy.inference import VariableElimination\n",
        "\n",
        "# Set up for Variable Elimination\n",
        "infer = VariableElimination(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkIyj0sOpM9B"
      },
      "outputs": [],
      "source": [
        "# Question 3 (Part 2) Compute P(Radiation | Cancer = 1)\n",
        "phi_query = infer.query(['Radiation'], evidence={'Cancer':1}, joint = False)\n",
        "factor = phi_query['Radiation']\n",
        "print('Probability of Radiation given Cancer = 1:')\n",
        "print(factor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JesATZ7UppfH"
      },
      "outputs": [],
      "source": [
        "# Question 3 (Part 3) Compute P(Cancer | Skin Burn = 1)\n",
        "phi_query = infer.query(['Cancer'], evidence={'Skin Burn':1}, joint = False)\n",
        "factor = phi_query['Cancer']\n",
        "print('Probability of Cancer given Skin Burn = 1:')\n",
        "print(factor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g01USXcOqT6x"
      },
      "outputs": [],
      "source": [
        "# Question 3 (Part 5) Compute P(Cancer = 1 | Microwave = 0)\n",
        "phi_query = infer.query(['Cancer'], evidence={'Microwave':0}, joint = False)\n",
        "factor = phi_query['Cancer']\n",
        "print('Probability of Cancer given Microwave = 0:')\n",
        "print(factor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqH-zvRYrIYG"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0eFoSe6DBTu"
      },
      "source": [
        "**Question 4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cc5J_jJHthA"
      },
      "outputs": [],
      "source": [
        "# Import the required libraries\n",
        "\n",
        "%matplotlib inline\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSHhKLe5K6os"
      },
      "outputs": [],
      "source": [
        "# (a) Download and load the k_mnist dataset.\n",
        "(train_images, train_labels), (test_images, test_labels) = tfds.as_numpy(tfds.load(\n",
        "    'kmnist',\n",
        "    split=['train', 'test'],\n",
        "    batch_size=-1,\n",
        "    as_supervised=True,\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfh6OdAPLyio"
      },
      "outputs": [],
      "source": [
        "# Reshape the images to required dimension\n",
        "train_images = train_images.reshape((60000, 28, 28))\n",
        "test_images = test_images.reshape((10000, 28, 28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQX4X3VqHtiM"
      },
      "outputs": [],
      "source": [
        "# Check shapes of train_images, train_labels etc\n",
        "print(train_images.shape)\n",
        "print(train_labels.shape)\n",
        "print(test_images.shape)\n",
        "print(test_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMOlhc70HTOr"
      },
      "outputs": [],
      "source": [
        "# Assign name to corresponding classes\n",
        "class_names = ['o', 'ki', 'su', 'tsu', 'na', 'ha', 'ma', 'ya', 're', 'wo']\n",
        "\n",
        "# Show first 25 training images below\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[train_labels[i]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIbA6kYprxIz"
      },
      "outputs": [],
      "source": [
        "# (b) Create ANN with :\n",
        "# 1 input layer\n",
        "# 3 hidden layers, with 128, 128, and 64 nodes each, and all using relu activation function\n",
        "# This configuration achieves overall accuracy of >80% on the testing dataset after training \n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(layers.Dense(128, activation=tf.nn.relu))\n",
        "model.add(layers.Dense(128, activation=tf.nn.relu))\n",
        "model.add(layers.Dense(64, activation=tf.nn.relu))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzCMoJi7s33C"
      },
      "outputs": [],
      "source": [
        "# (c) Create 1 output layer with:\n",
        "# size of 10\n",
        "# softmax activation function\n",
        "model.add(layers.Dense(10, activation=tf.nn.softmax))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6ySRwtHSVgM"
      },
      "outputs": [],
      "source": [
        "# (d) Compile the neural network with:\n",
        "# cross entropy loss function\n",
        "# Adamax optimizer with learning rate of 1Ã—10^-3\n",
        "model.compile(optimizer=tf.keras.optimizers.Adamax(learning_rate=1e-3),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvdjI4AhXC1s"
      },
      "outputs": [],
      "source": [
        "# Train the neural network\n",
        "# Run the stochastic gradient descent for batch size : 16 and epochs : 6  \n",
        "model.fit(train_images, train_labels, batch_size=16, epochs=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwXIlq5aXG_M"
      },
      "outputs": [],
      "source": [
        "# Evaluate the result of the model applied on the test images\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('Test accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aszt1Crcw2IK"
      },
      "outputs": [],
      "source": [
        "# Get all predictions for test data\n",
        "predictions = model.predict(test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84g64zxDoE97"
      },
      "outputs": [],
      "source": [
        "# Prediction visualization\n",
        "# Correct predictions are highlighted in green\n",
        "# Incorrect predictions are highlighted in red\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(test_images[i], cmap=plt.cm.binary)\n",
        "    predicted_label = np.argmax(predictions[i])\n",
        "    true_label = test_labels[i]\n",
        "    if predicted_label == true_label:\n",
        "      color = 'green'\n",
        "    else:\n",
        "      color = 'red'\n",
        "    plt.xlabel(\"{} ({})\".format(class_names[predicted_label], \n",
        "                                  class_names[true_label]),\n",
        "                                  color=color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSlzmFTbn55_"
      },
      "outputs": [],
      "source": [
        "# (e) Plot the average training error on the y-axis vs the epoch number \n",
        "\n",
        "# Simulate a similar training environment\n",
        "eval_model = tf.keras.Sequential()\n",
        "eval_model.add(layers.Flatten(input_shape=(28, 28)))\n",
        "eval_model.add(layers.Dense(128, activation=tf.nn.relu))\n",
        "eval_model.add(layers.Dense(128, activation=tf.nn.relu))\n",
        "eval_model.add(layers.Dense(64, activation=tf.nn.relu))\n",
        "eval_model.add(layers.Dense(10, activation=tf.nn.softmax))\n",
        "\n",
        "eval_model.compile(optimizer=tf.keras.optimizers.Adamax(learning_rate=1e-3),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "hist = eval_model.fit(train_images, train_labels, batch_size=16, epochs=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNethRL1xnLc"
      },
      "outputs": [],
      "source": [
        "# Extract the training metrics for evaluation\n",
        "hist_df = pd.DataFrame(hist.history)\n",
        "\n",
        "epoch = [1,2,3,4,5,6]\n",
        "hist_df['epoch no'] = epoch \n",
        "hist_df = hist_df.set_index('epoch no')\n",
        "\n",
        "hist_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVmOFAwRXPx2"
      },
      "outputs": [],
      "source": [
        "# Plot the result \n",
        "SAMPLE_NO = 60000\n",
        "\n",
        "hist_arr = hist_df.to_numpy()\n",
        "\n",
        "x = np.array([1,2,3,4,5,6])\n",
        "\n",
        "\n",
        "y = np.array([\n",
        "     hist_arr[0][0],\n",
        "     hist_arr[1][0],\n",
        "     hist_arr[2][0],\n",
        "     hist_arr[3][0],\n",
        "     hist_arr[4][0],\n",
        "     hist_arr[5][0],\n",
        "    ])\n",
        "\n",
        "\n",
        "y = y / SAMPLE_NO\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(x,y,color='red',marker='o')\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Avarage Training Error')\n",
        "plt.title(\"Average Training Error against Epoch Number\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg4dhXD6hzb8"
      },
      "outputs": [],
      "source": [
        "# (f) final accuracy of different classes and overall accuracy on the testing data\n",
        "\n",
        "predicted_classes = [0,0,0,0,0,0,0,0,0,0]\n",
        "expected_classes = [0,0,0,0,0,0,0,0,0,0]\n",
        "\n",
        "for i in range(10000):\n",
        "  exp_label = int(test_labels[i])\n",
        "  pre_label = int(np.argmax(predictions[i]))\n",
        "\n",
        "  if exp_label == pre_label:\n",
        "    predicted_classes[pre_label] += 1\n",
        "\n",
        "  expected_classes[exp_label] += 1\n",
        "\n",
        "print('-----Final accuracy for different classes-----')\n",
        "\n",
        "for i in range(10):\n",
        "  perc_val = \"{:.2f}\".format(predicted_classes[i]/expected_classes[i] * 100)\n",
        "  print(\"For label \" + str(i) + \" ('\" + str(class_names[i]) + \n",
        "        \"') : Accuracy is \" + str(perc_val) + \"%\\n\")\n",
        "  \n",
        "print('\\n\\n-----Final accuracy for test dataset-----')\n",
        "print('overall accuracy: ' + str(\"{:.2f}\".format(test_acc * 100)) + '\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUghWq1ck-5t"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ__YzNATUuM"
      },
      "source": [
        "**Question 5**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3wXL7ChhHdE"
      },
      "outputs": [],
      "source": [
        "# Import the required libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.applications.mobilenet_v2\n",
        "\n",
        "from keras import layers\n",
        "from keras.datasets import cifar10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMtaw6kVhHdJ"
      },
      "source": [
        "### Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9RqMF3hhHdJ",
        "outputId": "1c6af607-12f2-445f-8a08-56096873e5bc"
      },
      "outputs": [],
      "source": [
        "# Load training data, labels; and testing data and their true labels\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "print ('Training data seize:', train_images.shape)\n",
        "print ('Test data size', test_images.shape)\n",
        "\n",
        "# Normalize pixel values between -1 and 1\n",
        "train_images = train_images / 127.5 - 1 \n",
        "test_images = test_images / 127.5 - 1 \n",
        "\n",
        "# Class names for different classes\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer','dog', 'frog', 'horse', 'ship', 'truck']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK_IvOXcsLH_"
      },
      "outputs": [],
      "source": [
        "# Testing the GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F82ZGbcehHdK"
      },
      "source": [
        "### Visualize dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqBtlR42hHdK"
      },
      "outputs": [],
      "source": [
        "# Show first 25 training images below\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[train_labels[i][0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSH3eFZ-Qnzn"
      },
      "outputs": [],
      "source": [
        "# Check the shapes of train_images, train_labels, etc\n",
        "print(train_images.shape)\n",
        "print(train_labels.shape)\n",
        "print(test_images.shape)\n",
        "print(test_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpLF6OTMhHdK"
      },
      "source": [
        "### Resize images for use with MobileNetV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qxmpUIAZhHdK"
      },
      "outputs": [],
      "source": [
        "# Upsize all training and testing images to 96x96 for use with mobile net\n",
        "\n",
        "# Minimum size requried for mobileNetV2\n",
        "minSize = 96 \n",
        "\n",
        "# Initialize a numpy array resized_train_images to store all the resized training images\n",
        "resized_train_images = np.zeros((50000, minSize, minSize, 3), dtype=np.float32)\n",
        "\n",
        "# Resize the training images\n",
        "for i in range(50000):\n",
        "  resized_train_images[i] = cv2.resize(train_images[i], dsize=(minSize, minSize), interpolation=cv2.INTER_AREA)\n",
        " \n",
        "# Initialize a numpy array resized_test_images to store all the resized test images\n",
        "resized_test_images = np.zeros((10000, minSize, minSize, 3), dtype=np.float32)\n",
        "\n",
        "# Resize the test images\n",
        "for i in range(10000):\n",
        "  resized_test_images[i] = cv2.resize(test_images[i], dsize=(minSize, minSize), interpolation=cv2.INTER_AREA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mppCuZ1xsp9w"
      },
      "outputs": [],
      "source": [
        "print(resized_train_images.shape)\n",
        "print(train_images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQd9E-HthHdL"
      },
      "source": [
        "### Download MobileNetV2 model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRjD4JIGEQD-"
      },
      "outputs": [],
      "source": [
        "# (a) Download the pre-trained MobileNetV2 network from Keras Applications\n",
        "IMG_SHAPE = (96, 96, 3)\n",
        "\n",
        "# (b) Remove the final output layer of the network by setting include_top to False\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
        "\n",
        "# Freeze the layers in pre-trained model, except last 8 layers\n",
        "for layer in base_model.layers[:-8]:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Show the summary and read the number of trainable parameters\n",
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRr8E0yOhHdL"
      },
      "source": [
        "### Add custom layers at the end of downloaded model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwIh1w8iga9i"
      },
      "outputs": [],
      "source": [
        "# (c) Extend the MobileNetV2 model\n",
        "\n",
        "# Create a global average layer before connecting MobileNetV2 with new model\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(base_model)\n",
        "\n",
        "# Extend the MobileNetV2 model with:\n",
        "# 1 global average layer\n",
        "# 6 hidden dense layers, with 128, 64, 128, 64, 128, 64 nodes each, and all using relu activation function\n",
        "# 1 output layer with size of 10, and softmax activation function\n",
        "# This configuration achieves overall accuracy of >85% on the testing dataset after training\n",
        "\n",
        "model.add(global_average_layer)\n",
        "\n",
        "model.add(layers.Dense(128, activation=tf.nn.relu))\n",
        "model.add(layers.Dense(64, activation=tf.nn.relu))\n",
        "model.add(layers.Dense(128, activation=tf.nn.relu))\n",
        "model.add(layers.Dense(64, activation=tf.nn.relu))\n",
        "model.add(layers.Dense(128, activation=tf.nn.relu))\n",
        "model.add(layers.Dense(64, activation=tf.nn.relu))\n",
        "\n",
        "model.add(layers.Dense(10, activation=tf.nn.softmax))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFxL2Mk8hHdM"
      },
      "source": [
        "### Add loss function, compile and train the model, and check accuracy on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CmUZvWPN2iR"
      },
      "outputs": [],
      "source": [
        "# (d) Compile the neural network with:\n",
        "# cross-entropy loss function\n",
        "# Adamax optimizer with learning rate of 1x10^-3\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adamax(learning_rate=1e-3),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lgsro4qUH_0"
      },
      "outputs": [],
      "source": [
        "# Show the hierarchy of the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91txuhsVUH9g",
        "outputId": "9cb5ba0a-08a2-42b6-e12b-b861d700652e"
      },
      "outputs": [],
      "source": [
        "# Train the neural network \n",
        "# Run the stochastic gradient descent for batch size : 32 and epochs : 6\n",
        "epochs = 6\n",
        "batch_size = 32\n",
        "\n",
        "hist2 = model.fit(resized_train_images, train_labels, batch_size=batch_size, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1F5iPAyxUH7T",
        "outputId": "45358f04-7745-48e1-ae7c-89860fcdf2e2"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(resized_test_images, test_labels, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NArlbRc6UH6l",
        "outputId": "e3199121-45f8-49c3-c53e-c3497df1bb5a"
      },
      "outputs": [],
      "source": [
        "print('Test accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RL1L8h7iUH4s"
      },
      "outputs": [],
      "source": [
        "# Get all predictions for test data\n",
        "predictions = model.predict(resized_test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SXE5SF-mUH3o",
        "outputId": "dcd94c80-b690-4e5a-ac30-9cf24d70ba02"
      },
      "outputs": [],
      "source": [
        "# Code to visualize predictions\n",
        "# Incorrect predictions are highlighted in red\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(resized_test_images[i], cmap=plt.cm.binary)\n",
        "    predicted_label = np.argmax(predictions[i])\n",
        "    true_label = test_labels[i][0]\n",
        "    if predicted_label == true_label:\n",
        "      color = 'green'\n",
        "    else:\n",
        "      color = 'red'\n",
        "    plt.xlabel(\"{} ({})\".format(class_names[predicted_label], \n",
        "                                  class_names[true_label]),\n",
        "                                  color=color)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8HuDLIIhHdM"
      },
      "source": [
        "### Extra code for producing different plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "grsZI6rthHdM"
      },
      "outputs": [],
      "source": [
        "#Extract the training metrics for evaluation\n",
        "hist_df = pd.DataFrame(hist2.history)\n",
        "\n",
        "epoch = [1,2,3,4,5,6]\n",
        "hist_df['epoch no'] = epoch \n",
        "hist_df = hist_df.set_index('epoch no')\n",
        "\n",
        "# Plot the result \n",
        "SAMPLE_NO = 60000\n",
        "\n",
        "hist_arr = hist_df.to_numpy()\n",
        "\n",
        "x = np.array([1,2,3,4,5,6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "tQazUl6VHnVX",
        "outputId": "60325196-6b3d-42da-dfc8-9979cab81bf1"
      },
      "outputs": [],
      "source": [
        "y = np.array([\n",
        "     hist_arr[0][0],\n",
        "     hist_arr[1][0],\n",
        "     hist_arr[2][0],\n",
        "     hist_arr[3][0],\n",
        "     hist_arr[4][0],\n",
        "     hist_arr[5][0],\n",
        "    ])\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(x,y,color='green',marker='o')\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss Function Value')\n",
        "plt.title(\"Loss Function Value against Epoch Number\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2pTF8gc2SkO",
        "outputId": "547499a7-d836-422b-b68c-2921b777929a"
      },
      "outputs": [],
      "source": [
        "# Show the overall accuracy\n",
        "\n",
        "print('-----Final accuracy for test dataset-----')\n",
        "print('overall accuracy: ' + str(\"{:.2f}\".format(test_acc * 100)) + '\\n\\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "KANG CHIN SHEN_CS420 assignment1 code.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
